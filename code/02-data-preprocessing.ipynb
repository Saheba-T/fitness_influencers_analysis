{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "adcf37c6-831f-47d6-af97-05de94a42414",
   "metadata": {},
   "source": [
    "# Data Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "075e8e58-7afa-4d66-be51-6bf2c1a4d6f6",
   "metadata": {},
   "source": [
    "First, we are going to clean the data and then we are going to perform feature engineering to enrich the data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d7ed3e1-8316-489a-a048-5969b182f86b",
   "metadata": {},
   "source": [
    "## 1. Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f8bdfa8-8aa6-49cd-ad1c-59782ae8a5f5",
   "metadata": {
    "tags": []
   },
   "source": [
    "First, we are going to clean the data by:\n",
    "- checking for duplicate rows\n",
    "- checking for missing values\n",
    "- checking the percentage of missing values of each column\n",
    "- checking for any redundant (uninformative) columns\n",
    "- checking the column data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d2197680-4917-4213-8e56-60439f0875e9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import ast\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy         # Natural language processing\n",
    "import isodate       # Date transformation and manipulation\n",
    "\n",
    "from os.path import dirname, abspath\n",
    "from dateutil import parser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f6ff9c-5f7d-4998-bd8f-ec871652d564",
   "metadata": {},
   "source": [
    "#### Read the channels and videos files into dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ed0c9af1-fcca-42b0-9067-0356279a3396",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the csv files from the raw data folder\n",
    "project_dir = dirname(dirname(abspath(\"02-data-preprocessing.ipynb\")))\n",
    "channels_df = pd.read_csv(project_dir + \"/data/raw/fitness_channels_2023_07_11.csv\")\n",
    "videos_df = pd.read_csv(project_dir + \"/data/raw/fitness_videos_2023_07_11.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82e003d3-1aa4-42c5-a978-e953b7071580",
   "metadata": {},
   "source": [
    "#### Check for duplicate rows and columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0c40d1ab-68a3-4170-8c29-5c808b289dac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(False, False)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check for duplicate rows in channels and videos data\n",
    "(channels_df.duplicated().any(),videos_df.duplicated().any())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e2a9230-badf-4965-b561-b1f538e3aa50",
   "metadata": {},
   "source": [
    "#### Check for missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "08e50670-6306-48f2-ab61-8b5584360778",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChannelName           False\n",
       "ChannelDescription    False\n",
       "PublishedDate         False\n",
       "TotalSubscribers      False\n",
       "TotalViews            False\n",
       "TotalVideos           False\n",
       "playlistID            False\n",
       "dtype: bool"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "channels_df.isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fd67da71-e057-4a53-a1ac-cba14a5a0e40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "video_id          False\n",
       "channelTitle      False\n",
       "title             False\n",
       "description        True\n",
       "tags               True\n",
       "publishedAt       False\n",
       "viewCount         False\n",
       "likeCount          True\n",
       "favouriteCount     True\n",
       "commentCount       True\n",
       "duration          False\n",
       "definition        False\n",
       "dtype: bool"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "videos_df.isnull().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "677c8146-32bd-40e8-b72c-840deb7ed216",
   "metadata": {},
   "source": [
    "#### Check for percentage of missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "18d1bb60-f54f-4742-be70-417efef098c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "description         4.554311\n",
       "tags               12.883585\n",
       "likeCount           1.217730\n",
       "favouriteCount    100.000000\n",
       "commentCount        0.243546\n",
       "dtype: float64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find the percentage of missing values from columns that contains them in the videos dataframe\n",
    "missingval_columns = videos_df.loc[:, ['description', 'tags', 'likeCount','favouriteCount','commentCount']]\n",
    "missingval_columns.isnull().sum() / missingval_columns.shape[0] * 100.00"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "424698ba-81dd-45e9-97d0-208efaa15577",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the favouriteCount column since it only contains missing values\n",
    "videos_df.drop('favouriteCount', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70728b86-4158-4ed0-9b9c-959371da7722",
   "metadata": {},
   "source": [
    "#### Check for redundant columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c1ab53dc-ed38-4755-8569-58f0796e0148",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['hd', 'sd'], dtype=object)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find the unique values of the definition column\n",
    "videos_df['definition'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "71092279-0d60-4eff-8dba-77ace2374f3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "hd    99.561617\n",
       "sd     0.438383\n",
       "Name: definition, dtype: float64"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the percentage of each unique value in the definition column\n",
    "videos_df['definition'].value_counts() / videos_df['definition'].shape[0] * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7be914e8-3e29-4cf0-8dc0-33739086fabc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the definition column for being uninformative\n",
    "videos_df.drop('definition', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9568b410-d3d8-4442-8e1a-fb2db91516e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2836    2009-10-06T04:47:37Z\n",
       "2835    2009-11-04T18:05:19Z\n",
       "2834    2009-11-10T05:18:29Z\n",
       "2833    2009-11-18T06:49:44Z\n",
       "2832    2009-11-30T08:13:39Z\n",
       "                ...         \n",
       "1       2023-07-10T15:18:40Z\n",
       "733     2023-07-11T12:46:31Z\n",
       "1652    2023-07-11T13:00:16Z\n",
       "0       2023-07-11T14:00:17Z\n",
       "1241    2023-07-11T14:00:39Z\n",
       "Name: publishedAt, Length: 4106, dtype: object"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the date values to make sure there are no errors\n",
    "videos_df.publishedAt.sort_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d50fee46-6a54-4c0e-84ee-5b76d571b9ff",
   "metadata": {},
   "source": [
    "#### Checking the column data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "30047caf-1401-4e71-8328-abbfd24a54cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChannelName           object\n",
       "ChannelDescription    object\n",
       "PublishedDate         object\n",
       "TotalSubscribers       int64\n",
       "TotalViews             int64\n",
       "TotalVideos            int64\n",
       "playlistID            object\n",
       "dtype: object"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "channels_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "87e8c21f-ca90-4da4-a527-9d17c9eb9cc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "video_id         object\n",
       "channelTitle     object\n",
       "title            object\n",
       "description      object\n",
       "tags             object\n",
       "publishedAt      object\n",
       "viewCount         int64\n",
       "likeCount       float64\n",
       "commentCount    float64\n",
       "duration         object\n",
       "dtype: object"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "videos_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e00424cb-d964-4b1c-9bc6-37c7f47e77ee",
   "metadata": {},
   "source": [
    "## 2. Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e7bea67-de6b-4ac7-8236-0e50fba8ba19",
   "metadata": {},
   "source": [
    "Second, we are going to perform feature engineering to enrich the data by:\n",
    "* Convert ISO 8601 duration format from the YouTube Data API v3\n",
    "    1. first convert to timedelta[s]\n",
    "    2. then convert to minutes since this unit of duration is more intuitive to work with for workout videos\n",
    "    3. check for any errors or outliers (address them if necessary)\n",
    "* Derive new features from the published date column \n",
    "    1. Get published year\n",
    "    2. Get published month\n",
    "    3. Get published day of the week\n",
    "    4. Get published time of day\n",
    "* Get the length of the video title\n",
    "* Get the total number of tags in the tags column\n",
    "* use natural language processing (via spaCy) to derive useful features from the video title, description, and/or tags\n",
    "    1. Get the workout length; since actual workout duration and video duration are usually not the same\n",
    "    2. Get the workout type (cardio, HIIT, yoga, pilates, etc..)\n",
    "    3. Get the target body part (legs, abs, arms ,etc..)\n",
    "    3. Get any special aspect of the workout (no jumping, standing, no equipment, etc..)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e820695d-48bb-41a3-8f1e-bd5837e4b397",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert duration to seconds\n",
    "videos_df['durationSecs'] = videos_df['duration'].apply(lambda x: isodate.parse_duration(x))\n",
    "videos_df['durationMins'] = videos_df['durationSecs'].astype('timedelta64[s]')/60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "92ba4172-679c-4428-9ac8-5089fd40419f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    4106.000000\n",
       "mean       10.764178\n",
       "std         9.224501\n",
       "min         0.000000\n",
       "25%         3.650000\n",
       "50%        10.616667\n",
       "75%        14.491667\n",
       "max       130.816667\n",
       "Name: durationMins, dtype: float64"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the distribution of the video duration for presence of any outliers\n",
    "videos_df['durationMins'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1f36611c-8d9d-49a0-9ce3-09a0651dcb28",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>channelTitle</th>\n",
       "      <th>title</th>\n",
       "      <th>duration</th>\n",
       "      <th>video_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>822</th>\n",
       "      <td>emi wong</td>\n",
       "      <td>45 MIN WALKING CARDIO WORKOUT | Intense Full B...</td>\n",
       "      <td>PT45M18S</td>\n",
       "      <td>fIDmwKCJmlA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>930</th>\n",
       "      <td>emi wong</td>\n",
       "      <td>45 min Full Body Workout to BURN MAX CALORIES ...</td>\n",
       "      <td>PT47M58S</td>\n",
       "      <td>Wgm1Xc25imM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>991</th>\n",
       "      <td>emi wong</td>\n",
       "      <td>1 HOUR FULL BODY FAT BURN HOME WORKOUT (Warm U...</td>\n",
       "      <td>PT1H2M35S</td>\n",
       "      <td>p188evCXF0k</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1013</th>\n",
       "      <td>emi wong</td>\n",
       "      <td>2 Weeks Workout Program to Lose Weight, Get Ab...</td>\n",
       "      <td>PT1H4M51S</td>\n",
       "      <td>EJKw3Mh0MyI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1094</th>\n",
       "      <td>emi wong</td>\n",
       "      <td>45-min Full Body Fat Burn HIIT at home with NO...</td>\n",
       "      <td>PT46M43S</td>\n",
       "      <td>wAIRYalt75w</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1389</th>\n",
       "      <td>Chloe Ting</td>\n",
       "      <td>Abs &amp; Booty Workout Livestream</td>\n",
       "      <td>PT1H34M6S</td>\n",
       "      <td>tahd5q-onKc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1409</th>\n",
       "      <td>Chloe Ting</td>\n",
       "      <td>10 Million Subs LIVESTREAM | Let's hangout + W...</td>\n",
       "      <td>PT2H10M49S</td>\n",
       "      <td>IdO0Ie3_2QU</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1451</th>\n",
       "      <td>Chloe Ting</td>\n",
       "      <td>2000 REP Full Body &amp; Abs Workout CHALLENGE for...</td>\n",
       "      <td>PT49M48S</td>\n",
       "      <td>004CudS_3Ew</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1517</th>\n",
       "      <td>Chloe Ting</td>\n",
       "      <td>45 Min Full Body FAT BURN Workout | Get Flat A...</td>\n",
       "      <td>PT46M53S</td>\n",
       "      <td>LDvAuqTZxMw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1737</th>\n",
       "      <td>blogilates</td>\n",
       "      <td>POPFLEX Pre-Black Friday Extravaganza</td>\n",
       "      <td>PT1H12M46S</td>\n",
       "      <td>HO84VmEkfq0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2294</th>\n",
       "      <td>blogilates</td>\n",
       "      <td>*Special* Full Length 1 Hour POP Pilates Class!</td>\n",
       "      <td>PT1H8M47S</td>\n",
       "      <td>s0CkBw5Wock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2602</th>\n",
       "      <td>blogilates</td>\n",
       "      <td>Can I do 1000 Squats? FULL LENGTH Version</td>\n",
       "      <td>PT45M16S</td>\n",
       "      <td>fF71IJUXGOk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3047</th>\n",
       "      <td>Rebecca-Louise</td>\n",
       "      <td>FULL BODY FAT LOSS ðŸ”¥ 30 min at Home Workout + ...</td>\n",
       "      <td>PT55M16S</td>\n",
       "      <td>_NOGXxLGQ0A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3123</th>\n",
       "      <td>Rebecca-Louise</td>\n",
       "      <td>LEAN OUT Live Workout | Q&amp;A with Rebecca Louis...</td>\n",
       "      <td>PT45M35S</td>\n",
       "      <td>AKzN3-JBOvU</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3340</th>\n",
       "      <td>Rebecca-Louise</td>\n",
       "      <td>30 minutes FLAT TUMMY &amp; THIGH GAP at home work...</td>\n",
       "      <td>PT45M36S</td>\n",
       "      <td>hx6E8eTOmNs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3341</th>\n",
       "      <td>Rebecca-Louise</td>\n",
       "      <td>30 minute ROUND BOOTY &amp; SMALL WIAST workout (b...</td>\n",
       "      <td>PT46M</td>\n",
       "      <td>_pJTjFQLR_8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3366</th>\n",
       "      <td>Rebecca-Louise</td>\n",
       "      <td>LOSE FAT AND GET TONED in 40 minutes from home</td>\n",
       "      <td>PT47M35S</td>\n",
       "      <td>s0H3x9pH7yk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3621</th>\n",
       "      <td>Rebecca-Louise</td>\n",
       "      <td>Master the mental &amp; the POWER of visualization...</td>\n",
       "      <td>PT51M52S</td>\n",
       "      <td>0o4JRqqZmb4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3624</th>\n",
       "      <td>Rebecca-Louise</td>\n",
       "      <td>Know your value and OWN your self-worth | Rebe...</td>\n",
       "      <td>PT48M47S</td>\n",
       "      <td>G25aQB1sCWQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3642</th>\n",
       "      <td>Rebecca-Louise</td>\n",
       "      <td>Building a BUSINESS &amp; how to create VIRAL VIDE...</td>\n",
       "      <td>PT56M20S</td>\n",
       "      <td>wmF_P2QhB3M</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        channelTitle                                              title  \\\n",
       "822         emi wong  45 MIN WALKING CARDIO WORKOUT | Intense Full B...   \n",
       "930         emi wong  45 min Full Body Workout to BURN MAX CALORIES ...   \n",
       "991         emi wong  1 HOUR FULL BODY FAT BURN HOME WORKOUT (Warm U...   \n",
       "1013        emi wong  2 Weeks Workout Program to Lose Weight, Get Ab...   \n",
       "1094        emi wong  45-min Full Body Fat Burn HIIT at home with NO...   \n",
       "1389      Chloe Ting                     Abs & Booty Workout Livestream   \n",
       "1409      Chloe Ting  10 Million Subs LIVESTREAM | Let's hangout + W...   \n",
       "1451      Chloe Ting  2000 REP Full Body & Abs Workout CHALLENGE for...   \n",
       "1517      Chloe Ting  45 Min Full Body FAT BURN Workout | Get Flat A...   \n",
       "1737      blogilates              POPFLEX Pre-Black Friday Extravaganza   \n",
       "2294      blogilates    *Special* Full Length 1 Hour POP Pilates Class!   \n",
       "2602      blogilates          Can I do 1000 Squats? FULL LENGTH Version   \n",
       "3047  Rebecca-Louise  FULL BODY FAT LOSS ðŸ”¥ 30 min at Home Workout + ...   \n",
       "3123  Rebecca-Louise  LEAN OUT Live Workout | Q&A with Rebecca Louis...   \n",
       "3340  Rebecca-Louise  30 minutes FLAT TUMMY & THIGH GAP at home work...   \n",
       "3341  Rebecca-Louise  30 minute ROUND BOOTY & SMALL WIAST workout (b...   \n",
       "3366  Rebecca-Louise     LOSE FAT AND GET TONED in 40 minutes from home   \n",
       "3621  Rebecca-Louise  Master the mental & the POWER of visualization...   \n",
       "3624  Rebecca-Louise  Know your value and OWN your self-worth | Rebe...   \n",
       "3642  Rebecca-Louise  Building a BUSINESS & how to create VIRAL VIDE...   \n",
       "\n",
       "        duration     video_id  \n",
       "822     PT45M18S  fIDmwKCJmlA  \n",
       "930     PT47M58S  Wgm1Xc25imM  \n",
       "991    PT1H2M35S  p188evCXF0k  \n",
       "1013   PT1H4M51S  EJKw3Mh0MyI  \n",
       "1094    PT46M43S  wAIRYalt75w  \n",
       "1389   PT1H34M6S  tahd5q-onKc  \n",
       "1409  PT2H10M49S  IdO0Ie3_2QU  \n",
       "1451    PT49M48S  004CudS_3Ew  \n",
       "1517    PT46M53S  LDvAuqTZxMw  \n",
       "1737  PT1H12M46S  HO84VmEkfq0  \n",
       "2294   PT1H8M47S  s0CkBw5Wock  \n",
       "2602    PT45M16S  fF71IJUXGOk  \n",
       "3047    PT55M16S  _NOGXxLGQ0A  \n",
       "3123    PT45M35S  AKzN3-JBOvU  \n",
       "3340    PT45M36S  hx6E8eTOmNs  \n",
       "3341       PT46M  _pJTjFQLR_8  \n",
       "3366    PT47M35S  s0H3x9pH7yk  \n",
       "3621    PT51M52S  0o4JRqqZmb4  \n",
       "3624    PT48M47S  G25aQB1sCWQ  \n",
       "3642    PT56M20S  wmF_P2QhB3M  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for videos that are extremely long, greater than 45 mins.\n",
    "# Note: we get that there are 20 videos that are that long\n",
    "videos_df[videos_df['durationMins'] > 45][['channelTitle','title','duration','video_id']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "91b4d351-54b6-43ae-8368-18a10e0a02ef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Check for videos that are have 0 mins as duration.\n",
    "# there are livestreams, that looks like might have accidentally got turned on \n",
    "# and was turned off right away, so we are dropping those videos\n",
    "videos_df = videos_df[videos_df['durationMins'] > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6fdbbad8-a1d5-474e-b2e9-6ea4ee283120",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create publish datetime and year for channels data\n",
    "channels_df['publishedDatetime'] =  channels_df['PublishedDate'].apply(lambda x: parser.parse(x))\n",
    "channels_df['publishedYear'] = channels_df['publishedDatetime'].apply(lambda x: int(x.strftime(\"%Y\")))\n",
    "\n",
    "# Create publish year and month (of the year) columns\n",
    "videos_df['publishedDatetime'] = videos_df['publishedAt'].apply(lambda x: parser.parse(x))\n",
    "videos_df['publishedYear'] = videos_df['publishedDatetime'].apply(lambda x: int(x.strftime(\"%Y\")))\n",
    "videos_df['publishedMonth'] = videos_df['publishedDatetime'].apply(lambda x: x.strftime(\"%b\"))\n",
    "\n",
    "# Create publish day (of the week) column\n",
    "videos_df['pushblishDayName'] = videos_df['publishedDatetime'].apply(lambda x: x.strftime(\"%a\")) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7fb05ca4-520a-4b9a-9bf4-64bb00028cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the description column to string\n",
    "videos_df['description'] = videos_df['description'].astype(str)\n",
    "\n",
    "# Title character length\n",
    "videos_df['titleLength'] = videos_df['title'].apply(lambda x: len(x))\n",
    "\n",
    "# Create the number of tags column\n",
    "videos_df['tags'] = videos_df['tags'].replace(np.nan, None)\n",
    "videos_df['tags'] = videos_df['tags'].apply(lambda x: x if x is None else ast.literal_eval(x))\n",
    "videos_df['tagsCount'] = videos_df['tags'].apply(lambda x: 0 if x is None else len(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "dd073096-afcd-49b6-b1a9-e88651905b3d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Now we are going to use spaCy's entity ruler along with regex \n",
    "# to look for specific entities in the text columns.\n",
    "\n",
    "# Download spaCy's small english model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Create and add the EntityRuler\n",
    "ruler = nlp.add_pipe(\"entity_ruler\", before=\"ner\")\n",
    "\n",
    "#List of Entities and Patterns\n",
    "patterns = [\n",
    "    # labels for body part target\n",
    "    {\"label\": \"FULL_BODY\", \"pattern\": [{\"LOWER\": {\"REGEX\": r\"(full|total|whole)\"}}, {\"LOWER\": \"body\"}]},\n",
    "    {\"label\": \"UPPER_BODY\", \"pattern\": [{\"LOWER\": \"upper\"}, {\"LOWER\": \"body\"}]},\n",
    "    {\"label\": \"LOWER_BODY\", \"pattern\": [{\"LOWER\": \"lower\"}, {\"LOWER\": \"body\"}]},\n",
    "    {\"label\": \"CHEST\", \"pattern\": [{\"LOWER\": \"chest\"}]},\n",
    "    {\"label\": \"BACK\", \"pattern\": [{\"LOWER\": \"back\"}]},\n",
    "    {\"label\": \"ABS\", \"pattern\": [{\"LOWER\": {\"REGEX\": r\"(core|ab|abs|plank)\"}}]},\n",
    "    {\"label\": \"ARMS\", \"pattern\": [{\"LOWER\": {\"REGEX\": r\"arms?\"}}]},\n",
    "    {\"label\": \"LEGS\", \"pattern\": [{\"LOWER\": {\"REGEX\": r\"(thigh|thighs|leg|legs)\"}}]},\n",
    "    {\"label\": \"GLUTES\", \"pattern\": [{\"LOWER\": {\"REGEX\": r\"(booty|glute|glutes|butt)\"}}]},\n",
    "    # labels for workout type\n",
    "    {\"label\": \"HIIT\", \"pattern\": [{\"LOWER\": \"hiit\"}]},\n",
    "    {\"label\": \"CARDIO\", \"pattern\": [{\"LOWER\": \"cardio\"}]},\n",
    "    {\"label\": \"DANCE\", \"pattern\": [{\"LOWER\": \"dance\"}]},\n",
    "    {\"label\": \"TABATA\", \"pattern\": [{\"LOWER\": \"tabata\"}]},\n",
    "    {\"label\": \"PILATES\", \"pattern\": [{\"LOWER\": \"pilates\"}]},\n",
    "    {\"label\": \"BARRE\", \"pattern\": [{\"LOWER\": \"barre\"}]},\n",
    "    {\"label\": \"YOGA\", \"pattern\": [{\"LOWER\": \"yoga\"}]},\n",
    "    {\"label\": \"STANDING\", \"pattern\": [{\"LOWER\": \"standing\"}]},\n",
    "    {\"label\": \"NO_EQUIPMENT\", \"pattern\": [{\"LOWER\": \"no\", \"LOWER\": {\"REGEX\": r\"(equip|equipment|equipments|weight|weights)\"}}]},\n",
    "    {\"label\": \"NO_JUMPING\", \"pattern\": [{\"LOWER\": \"no\", \"LOWER\": \"jumping\"}]},\n",
    "    {\"label\": \"LOW_IMPACT\", \"pattern\": [{\"LOWER\": \"low\", \"LOWER\": \"impact\"}]},\n",
    "    {\"label\": \"STRENGTH_TRAINING\", \"pattern\": [{\"LOWER\": {\"REGEX\": r\"(strength|sculpt|sculpting|tone|toning|toned)\"}}]}\n",
    "]\n",
    "\n",
    "ruler.add_patterns(patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5e7515c5-09b3-4741-9488-34e72197d4d7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def extract_ent_label(string, label):\n",
    "    \"\"\" Identify presence of label using regular expressions\n",
    "\n",
    "    Arguments:\n",
    "        string -- the string value of either the title, description columns\n",
    "        label -- can be either one of the labels mentioned in the patterns list above\n",
    "\n",
    "    Returns:\n",
    "        A logical value, True if the label is found, else the default value is False\n",
    "    \"\"\"\n",
    "    doc = nlp(string)\n",
    "    result = False\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ == label:\n",
    "            result = True\n",
    "            break\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "05eb8037",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting video text information for column: FULL_BODY\n",
      "Getting video text information for column: UPPER_BODY\n",
      "Getting video text information for column: LOWER_BODY\n",
      "Getting video text information for column: CHEST\n",
      "Getting video text information for column: BACK\n",
      "Getting video text information for column: ABS\n",
      "Getting video text information for column: ARMS\n",
      "Getting video text information for column: LEGS\n",
      "Getting video text information for column: GLUTES\n",
      "Getting video text information for column: HIIT\n",
      "Getting video text information for column: CARDIO\n",
      "Getting video text information for column: DANCE\n",
      "Getting video text information for column: TABATA\n",
      "Getting video text information for column: PILATES\n",
      "Getting video text information for column: BARRE\n",
      "Getting video text information for column: YOGA\n",
      "Getting video text information for column: STANDING\n",
      "Getting video text information for column: NO_EQUIPMENT\n",
      "Getting video text information for column: NO_JUMPING\n",
      "Getting video text information for column: LOW_IMPACT\n",
      "Getting video text information for column: STRENGTH_TRAINING\n"
     ]
    }
   ],
   "source": [
    "# Extract the values for the \"label\" key and store them as a list\n",
    "labels_list = [pattern['label'] for pattern in patterns]\n",
    "\n",
    "# Create new columns to the exting videos dataframe\n",
    "videos_df = videos_df.assign(**{label: None for label in labels_list})\n",
    "\n",
    "# Apply the extract_ent_label function to the new columns row-wise using apply and lambda function\n",
    "for label in labels_list:\n",
    "    print(\"Getting video text information for column: \" + label)\n",
    "    \n",
    "    # extract text information about the workout from the video title first\n",
    "    videos_df[label] = videos_df.apply(lambda row: extract_ent_label(row['title'], label), axis=1)\n",
    "\n",
    "    # if missing from the title, extract text information about the workout from the video description\n",
    "    videos_df[label] = videos_df.apply(lambda row: extract_ent_label(row['description'], label)\\\n",
    "        if pd.isna(row[label]) else row[label], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d66e1fae-a080-4cbe-92a2-322ad8efaf11",
   "metadata": {},
   "source": [
    "#### Save the processed data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6c54b331-8415-4197-a0d1-75f12f914e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save dataframes as csv files in the processed subfolder of the data folder\n",
    "channels_df.to_csv(project_dir + \"/data/processed/fitness_channels_processed_2023_07_11.csv\", index=False)\n",
    "videos_df.to_csv(project_dir + \"/data/processed/fitness_videos_processed_2023_07_11.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
